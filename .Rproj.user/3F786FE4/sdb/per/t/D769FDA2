{
    "contents" : "# LOAD TWITTER PACKAGE\nrequire(tm)\nrequire(twitteR)\nrequire(ROAuth)\n\n# LOAD TWITTER CREDENTIALS\nt.api.key = \"pEMnGgZN2g61MsFGbdGOkX5RD\"\nt.api.secret = \"Sq9Z6VSm7PMuc6c9B75dunkg5StBQtzjxTa9BHkhsqjnTxxsl4\"\nt.api.accesstoken = \"2798804485-2aivg8zTXPNWcubok7JJWpHsocNCNnFb6U2H4IK\"\nt.api.accesssecret = \"GW9c5ZUs3mKku7fLb3Vgre0j8L60CG1FhjIxxhdcA6rgg\"\n\nt.cred <- load(\"twitcred\")\nsetup_twitter_oauth(t.api.key, t.api.secret, t.api.accesstoken, t.api.accesssecret)\n\n# A. GET TWEETS OF GAINERS\ntwtCNYD <- searchTwitter('$CNYD', n = 100)\ntwtBPTH <- searchTwitter('$BPTH', n = 100)\ntwtXPO <- searchTwitter('$XPO', n = 100)\nsave(twtCNYD, file=\"twtCNYD\")\nsave(twtBPTH, file=\"twtBPTH\")\nsave(twtXPO, file=\"twtXPO\")\ntwtCNYD = load(\"twtCNYD\")\ntwtBPTH = load(\"twtBPTH\")\ntwtXPO = load(\"twtXPO\")\n\ngainers = c(twtCNYD,twtBPTH,twtXPO)\n\n# A. GET TWEETS OF LOSERS\ntwtIG <- searchTwitter('$IG', n = 100)\ntwtSSYS <- searchTwitter('$SSYS', n = 100)\ntwtNVDQ <- searchTwitter('$NVDQ', n = 100)\nsave(twtIG, file=\"twtIG\")\nsave(twtSSYS, file=\"twtSSYS\")\nsave(twtNVDQ, file=\"twtNVDQ\")\ntwtIG = load(\"twtIG\")\ntwtSSYS = load(\"twtSSYS\")\ntwtNVDQ = load(\"twtNVDQ\")\n\nlosers = c(twtIG,twtSSYS,twtNVDQ)\n\n# B. CREATE TWO DATA CORPUS FOR THE TWEETS\ngetCorpus <- function (tweets) {\n  tweets.text <- lapply(tweets, function(t) {t$getText()})\n  data.source <- VectorSource(tweets.text)\n  data.corpus <- Corpus(data.source)\n  return (data.corpus)\n}\n\ngainers.corp = getCorpus(gainers)\ngainers.df<-data.frame(text=unlist(sapply(gainers.corp, `[`, \"content\")),stringsAsFactors=F)\nwriteCorpus(gainers.corp,path=paste0(getwd(),\"/data.corpus1\",sep=\"\"), filenames=NULL)\n\nlosers.corp = getCorpus(losers)\nlosers.df<-data.frame(text=unlist(sapply(losers.corp, `[`, \"content\")),stringsAsFactors=F)\nwriteCorpus(losers.corp,path=paste0(getwd(),\"/data.corpus2\",sep=\"\"), filenames=NULL)\n\n# C. PRE-PROCESS DATA\ngetTransCorpus <- function (data.corpus) {\n  data.corpus <- tm_map(data.corpus, content_transformer(removePunctuation))\n  data.corpus <- tm_map(data.corpus, content_transformer(tolower))\n  data.corpus <- tm_map(data.corpus, removeNumbers)\n  english.stopwords <- stopwords(\"en\")\n  removeURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\n  data.corpus <- tm_map(data.corpus, content_transformer(removeURL))\n  data.corpus <- tm_map(data.corpus,content_transformer(removeWords),english.stopwords)\n  data.corpus <- tm_map(data.corpus,content_transformer(stemDocument))\n  data.corpus <- tm_map(data.corpus,content_transformer(stripWhitespace))\n  \n  return (data.corpus)\n}\n\ntgainers.corp = getTransCorpus(gainers.corp)\ntlosers.corp = getTransCorpus(losers.corp)\n\n# D. CREATE TERM DOCUMENT MATRIX FOR EACH TWEET\ntdmGainers <- TermDocumentMatrix(tgainers.corp)\ntdmLosers <- TermDocumentMatrix(tlosers.corp)\n\nsave(tdmGainers, file=\"tdmGainers\")\nsave(tdmLosers, file=\"tdmLosers\")\n\n# E. COMPARE FREQ TERMS IN EACH STATE\nmGain <- as.matrix(tdmGainers)\nmLose <- as.matrix(tdmLosers)\n\n# CALCULATE FREQUENCY OF WORDS\nWFGain <- rowSums(mGain)\nWFGain = sort(WFGain,decreasing = T)\n\nWFLose <- rowSums(mLose)\nWFLose = sort(WFLose,decreasing = T)\n\n# FREQUENCIES OF ALL WORDS\ncbind(WFGain[1:750])\ncbind(WFLose[1:951])\n\n# FREQ TERMS WITH LOWER FREQ BOUND OF 10\nfindFreqTerms(tdmGainers, lowfreq=10)\nfindFreqTerms(tdmLosers, lowfreq=10)\n\n# GRAPHICAL WORDCLOUD OF THE FREQ WORDS\nrequire(wordcloud)\npalette = brewer.pal(8,\"Dark2\")\npar(mfrow=c(1,2))\nwordcloud(words = names(WFGain),\n          freq=WFGain,\n          min.freq=5,\n          random.order=F,\n          colors=palette)\n\nwordcloud(words = names(WFLose),\n          freq=WFLose,\n          min.freq=5,\n          random.order=F,\n          colors=palette)\npar(mfrow=c(1,1))\n\n# F. SENTIMENT ANALYSIS SCORE\npos.words = scan('positive-words.txt',\n                 what='character',\n                 comment.char=';')\n\nneg.words = scan('negative-words.txt',  \n                 what='character', \n                 comment.char=';')\n\nsentiment.na <- function(text, pos.words, neg.words) {\n  text <- gsub('[[:punct:]]', '', text)\n  text <- gsub('[[:cntrl:]]', '', text)\n  text <- gsub('\\\\d+', '', text)\n  text <- tolower(text)\n  # split the text into a vector of words\n  words <- strsplit(text, '\\\\s+')\n  words <- unlist(words)\n  # find which words are positive\n  pos.matches <- match(words, pos.words)\n  pos.matches <- !is.na(pos.matches)\n  # find which words are negative\n  neg.matches <- match(words, neg.words)\n  neg.matches <- !is.na(neg.matches)\n  # calculate the sentiment score\n  p <- sum(pos.matches)\n  n <- sum(neg.matches)\n  if (p == 0 & n == 0)\n    return (NA)\n  else\n    return (p - n)\n}\n\n# FETCH TEXT FROM TWEETS\ngain.texts <- \n  lapply(gainers, \n         function(t) {\n           iconv(t$getText(), \n                 \"latin1\", \"ASCII\", sub=\"\")\n         })\n\nlose.texts <- \n  lapply(losers, \n         function(t) {\n           iconv(t$getText(), \n                 \"latin1\", \"ASCII\", sub=\"\")\n         })\n\n\n# GET SCORES\ngain.scores.na <- sapply(gain.texts, \n                         sentiment.na, \n                         pos.words, neg.words)\n\nlose.scores.na <- sapply(lose.texts, \n                         sentiment.na, \n                         pos.words, neg.words)\n\ntable(gain.scores.na)\n\ntable(lose.scores.na)\n\npar(mfrow=c(1,2))\n\nbarplot(table(gain.scores.na), main=\"Sentiment Analysis for Gainers\",\n        xlab=\"Score\", ylab=\"Count\",\n        ylim=c(0,50), col=\"cyan\")\ngrid(nx=NA,ny=NULL,col=rgb(165,165,165,max=255),lty=1)\n\nbarplot(table(lose.scores.na), main=\"Sentiment Analysis for Losers\",\n        xlab=\"Score\", ylab=\"Count\",\n        ylim=c(0,50), col=\"cyan\")\ngrid(nx=NA,ny=NULL,col=rgb(165,165,165,max=255),lty=1)\n\npar(mfrow=c(1,1))\n\n# Data frame of scores and tweets\n\ngain.vector <- sapply(gain.texts,function (t) {(t)})\ngains <- data.frame(Score=gain.scores.na, Text=gain.vector)\nView(gains)\n\nlose.vector <- sapply(lose.texts,function (t) {(t)})\nloses <- data.frame(Score=lose.scores.na, Text=lose.vector)\nView(loses)\n\n# EXTRA CREDITS\nrequire(quantmod)\nlibrary(\"googleVis\", lib.loc=\"~/R/win-library/3.1\")\n\ngetSymbols(\"CNYD\")  #highest gainer\ngetSymbols(\"RCKY\")  #lowest gainer\n\n# FETCH THE DATA FOR THE DAY \ncnyd.df = data.frame(CNYD['2015-04-29'])\nrcky.df = data.frame(RCKY['2015-04-29'])\n\nopen = c(as.numeric(cnyd.df$CNYD.Open),as.numeric(rcky.df$RCKY.Open))\nhigh = c(as.numeric(cnyd.df$CNYD.High),as.numeric(rcky.df$RCKY.High))\nlow = c(as.numeric(cnyd.df$CNYD.Low),as.numeric(rcky.df$RCKY.Low))\nclose = c(as.numeric(cnyd.df$CNYD.Close),as.numeric(rcky.df$RCKY.Close))\nstocks = c(\"CNYD\",\"RCKY\")\n\nmergedrows = data.frame(stocks,open,high,low,close)\nstr(mergedrows)\n\nchartA = gvisBarChart(mergedrows,\n                        xvar=\"stocks\",\n                        yvar=c(\"open\",\"high\",\"low\",\"close\"))\nplot(chartA)\n",
    "created" : 1430340543979.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4293606782",
    "id" : "D769FDA2",
    "lastKnownWriteTime" : 1430456781,
    "path" : "C:/Stuff/Downloads/MS BSAN/WA/Project Report/Code/project.R",
    "project_path" : "project.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}